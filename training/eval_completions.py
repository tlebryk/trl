"""
Evaluate code completions by compiling and running them.

This script takes a JSON file of completions (generated by modal_inference.py)
and evaluates them by:
1. Compiling with g++ and sanitizers
2. Running the executable
3. Computing metrics: compile rate, runtime success rate, pass@k

Can run locally (no GPU needed) or on Modal.
"""
import json
import subprocess
import tempfile
import os
import sys
import math
from pathlib import Path
from typing import Dict, List, Optional
from collections import defaultdict

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from cpp_pipeline.cpp_utils import run_cpp_executable


def compile_and_run_code(code: str, timeout: float = 5.0) -> Dict:
    """
    Compile and run C++ code, return results.

    Returns:
        Dict with:
        - compiled: bool
        - compile_error: str (if compilation failed)
        - runtime_clean: bool
        - runtime_error: str (if runtime failed)
    """
    result = {
        "compiled": False,
        "runtime_clean": False,
    }

    # Write code to temp file
    with tempfile.NamedTemporaryFile(mode='w', suffix='.cpp', delete=False) as f:
        f.write(code)
        src_path = f.name

    exe_path = src_path.replace('.cpp', '.exe')

    try:
        # Compile with sanitizers
        compile_result = subprocess.run(
            ["g++", "-std=c++17", "-g", "-fsanitize=address", "-fsanitize=undefined",
             src_path, "-o", exe_path],
            capture_output=True,
            text=True,
            timeout=10
        )

        if compile_result.returncode == 0:
            result["compiled"] = True

            # Run executable
            success, stdout, stderr, runtime_errors = run_cpp_executable(exe_path, timeout=timeout)

            if success:
                result["runtime_clean"] = True
            else:
                result["runtime_error"] = stderr[:500] if stderr else "Unknown runtime error"
        else:
            result["compile_error"] = compile_result.stderr[:500] if compile_result.stderr else "Compilation failed"

    except subprocess.TimeoutExpired:
        result["compile_error"] = "Compilation timeout"
    except Exception as e:
        result["compile_error"] = str(e)
    finally:
        # Cleanup
        try:
            if os.path.exists(src_path):
                os.unlink(src_path)
            if os.path.exists(exe_path):
                os.unlink(exe_path)
        except:
            pass

    return result


def calculate_pass_at_k(n: int, c: int, k: int) -> float:
    """
    Calculate pass@k metric.

    Args:
        n: total number of samples
        c: number of correct samples
        k: k in pass@k

    Returns:
        pass@k probability
    """
    if n - c < k:
        return 1.0
    return 1.0 - math.prod((n - c - i) / (n - i) for i in range(k))


def evaluate_completions(completions_path: str, output_path: Optional[str] = None) -> Dict:
    """
    Evaluate completions from a JSON file.

    Args:
        completions_path: Path to completions JSON file
        output_path: Path to save evaluation results (optional)

    Returns:
        Dict with evaluation results
    """
    print("="*80)
    print("EVALUATING COMPLETIONS")
    print("="*80)
    print(f"Completions file: {completions_path}")

    # Load completions
    with open(completions_path, 'r') as f:
        completions_data = json.load(f)

    metadata = completions_data["metadata"]
    problems = completions_data["problems"]

    print(f"\nMetadata:")
    print(f"  Model: {metadata.get('model_id', 'unknown')}")
    print(f"  Model type: {metadata.get('model_type', 'unknown')}")
    print(f"  Adapter: {metadata.get('adapter_path', 'None')}")
    print(f"  Problems: {len(problems)}")
    print(f"  Samples per problem: {metadata.get('num_samples', 'unknown')}")
    print(f"  Temperature: {metadata.get('temperature', 'unknown')}")

    # Evaluate each completion
    eval_results = {
        "metadata": metadata,
        "problems": [],
        "summary": {}
    }

    total_samples = 0
    total_compiled = 0
    total_runtime_clean = 0
    problems_with_any_pass = 0

    print("\n" + "="*80)
    print("RUNNING EVALUATION")
    print("="*80)

    for problem_idx, problem in enumerate(problems):
        problem_name = problem["name"]
        prompt = problem["prompt"]
        tests = problem.get("tests", "")
        completions = problem["completions"]

        print(f"\n[{problem_idx+1}/{len(problems)}] {problem_name}")
        print("-" * 80)

        problem_result = {
            "name": problem_name,
            "samples": [],
            "compile_rate": 0.0,
            "runtime_rate": 0.0,
            "passed_any": False
        }

        samples_compiled = 0
        samples_runtime_clean = 0

        for completion in completions:
            sample_idx = completion["sample_idx"]
            generated_code = completion["generated_code"]

            # Combine prompt + generated code + tests
            full_code = prompt + generated_code
            if tests:
                full_code += "\n" + tests

            # Compile and run
            result = compile_and_run_code(full_code)

            total_samples += 1
            if result["compiled"]:
                samples_compiled += 1
                total_compiled += 1

            if result["runtime_clean"]:
                samples_runtime_clean += 1
                total_runtime_clean += 1

            # Store result
            sample_result = {
                "sample_idx": sample_idx,
                "compiled": result["compiled"],
                "runtime_clean": result["runtime_clean"],
            }

            if "compile_error" in result:
                sample_result["compile_error"] = result["compile_error"]
            if "runtime_error" in result:
                sample_result["runtime_error"] = result["runtime_error"]

            problem_result["samples"].append(sample_result)

            # Progress indicator
            status = "✓✓" if result["runtime_clean"] else ("✓✗" if result["compiled"] else "✗✗")
            print(f"  Sample {sample_idx+1}/{len(completions)}: {status}", end="\r")

        print()  # New line after progress

        # Problem-level metrics
        problem_result["compile_rate"] = samples_compiled / len(completions)
        problem_result["runtime_rate"] = samples_runtime_clean / len(completions)
        problem_result["passed_any"] = samples_runtime_clean > 0

        if problem_result["passed_any"]:
            problems_with_any_pass += 1

        print(f"  Compile: {samples_compiled}/{len(completions)} ({problem_result['compile_rate']:.1%})")
        print(f"  Runtime Clean: {samples_runtime_clean}/{len(completions)} ({problem_result['runtime_rate']:.1%})")

        eval_results["problems"].append(problem_result)

    # Calculate summary metrics
    num_samples_per_problem = metadata.get("num_samples", 10)

    # Calculate pass@k for different k values
    pass_at_k_metrics = {}
    for k in [1, 5, 10]:
        if k <= num_samples_per_problem:
            # Average pass@k across all problems
            pass_at_k_values = []
            for problem_result in eval_results["problems"]:
                n = len(problem_result["samples"])
                c = sum(1 for s in problem_result["samples"] if s["runtime_clean"])
                pass_at_k_values.append(calculate_pass_at_k(n, c, k))

            pass_at_k_metrics[f"pass@{k}"] = sum(pass_at_k_values) / len(pass_at_k_values) if pass_at_k_values else 0

    eval_results["summary"] = {
        "total_samples": total_samples,
        "compile_success_rate": total_compiled / total_samples if total_samples > 0 else 0,
        "runtime_success_rate": total_runtime_clean / total_samples if total_samples > 0 else 0,
        "problems_with_any_pass": problems_with_any_pass,
        "problems_with_any_pass_rate": problems_with_any_pass / len(problems) if len(problems) > 0 else 0,
        **pass_at_k_metrics
    }

    # Print summary
    print("\n" + "="*80)
    print("EVALUATION SUMMARY")
    print("="*80)
    print(f"Total samples: {total_samples}")
    print(f"Compilation success rate: {eval_results['summary']['compile_success_rate']:.2%}")
    print(f"Runtime success rate (ASan/UBSan clean): {eval_results['summary']['runtime_success_rate']:.2%}")
    for k, v in pass_at_k_metrics.items():
        print(f"{k}: {v:.2%}")
    print(f"Problems with ≥1 passing sample: {problems_with_any_pass}/{len(problems)} ({eval_results['summary']['problems_with_any_pass_rate']:.2%})")
    print("="*80)

    # Save results
    if output_path is None:
        # Auto-generate output path
        completions_filename = Path(completions_path).stem
        output_path = str(Path(completions_path).parent / f"eval_{completions_filename}.json")

    with open(output_path, "w") as f:
        json.dump(eval_results, f, indent=2)

    print(f"\n✅ Results saved to: {output_path}")

    return eval_results


def main():
    """
    Main CLI for evaluation.

    Usage:
        python training/eval_completions.py <completions.json> [output.json]

    Examples:
        # Evaluate completions (auto-generates output filename)
        python training/eval_completions.py ./completions_base.json

        # Evaluate with custom output path
        python training/eval_completions.py ./completions_base.json ./eval_results.json
    """
    if len(sys.argv) < 2:
        print("Usage: python training/eval_completions.py <completions.json> [output.json]")
        sys.exit(1)

    completions_path = sys.argv[1]
    output_path = sys.argv[2] if len(sys.argv) > 2 else None

    if not os.path.exists(completions_path):
        print(f"Error: Completions file not found: {completions_path}")
        sys.exit(1)

    evaluate_completions(completions_path, output_path)


if __name__ == "__main__":
    main()
