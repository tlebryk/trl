"""
Evaluate code completions by compiling and running them.

This script takes a JSON file of completions (generated by modal_inference.py)
and evaluates them by:
1. Compiling with g++ and sanitizers
2. Running the executable
3. Computing metrics: compile rate, runtime success rate, pass@k

Can run locally (no GPU needed) or on Modal.
"""
import json
import subprocess
import os
import sys
import math
import logging
from pathlib import Path
from typing import Dict, List, Optional
from collections import defaultdict

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from cpp_pipeline.cpp_utils import run_cpp_executable, clean_generated_code


def compile_and_run_code(code: str, src_path: str, timeout: float = 5.0) -> Dict:
    """
    Compile and run C++ code, return results.

    Args:
        code: C++ source code to compile
        src_path: Path where the source file should be saved (and compiled from)
        timeout: Runtime timeout in seconds

    Returns:
        Dict with:
        - compiled: bool
        - compile_error: str (if compilation failed)
        - runtime_clean: bool
        - runtime_error: str (if runtime failed)
        - src_file: str (path to saved source file)
        - exe_file: str (path to executable, if compiled)
    """
    result = {
        "compiled": False,
        "runtime_clean": False,
        "src_file": src_path,
    }

    # Write code to file (create parent directory if needed)
    os.makedirs(os.path.dirname(src_path), exist_ok=True)
    with open(src_path, 'w') as f:
        f.write(code)

    exe_path = src_path.replace('.cpp', '.exe')
    result["exe_file"] = exe_path

    try:
        # Get compiler from env or default to g++-15 (local) or g++ (modal)
        compiler = os.environ.get("CXX", "g++")
        logging.debug(f"Compiling with compiler: {compiler}")
        # Compile with sanitizers
        command = [compiler, "-std=c++17", "-g", "-fsanitize=address", "-fsanitize=undefined",
             src_path, "-o", exe_path]

        compile_result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            timeout=10,
        )
        print(f"command = {' '.join(command)}")
        print(f"compile_result = {compile_result}")

        if compile_result.returncode == 0:
            result["compiled"] = True

            # Run executable
            success, stdout, stderr, runtime_errors = run_cpp_executable(exe_path, timeout=timeout)

            if success:
                result["runtime_clean"] = True
            else:
                result["runtime_error"] = stderr[:500] if stderr else "Unknown runtime error"
        else:
            result["compile_error"] = compile_result.stderr[:500] if compile_result.stderr else "Compilation failed"

    except subprocess.TimeoutExpired:
        result["compile_error"] = "Compilation timeout"
    except Exception as e:
        result["compile_error"] = str(e)

    # Note: We don't delete files - they are saved as artifacts

    return result


def calculate_pass_at_k(n: int, c: int, k: int) -> float:
    """
    Calculate pass@k metric.

    Args:
        n: total number of samples
        c: number of correct samples
        k: k in pass@k

    Returns:
        pass@k probability
    """
    if n - c < k:
        return 1.0
    return 1.0 - math.prod((n - c - i) / (n - i) for i in range(k))


def evaluate_completions(completions_path: str, output_path: Optional[str] = None) -> Dict:
    """
    Evaluate completions from a JSON file.

    Args:
        completions_path: Path to completions JSON file
        output_path: Path to save evaluation results (optional)

    Returns:
        Dict with evaluation results
    """
    print("="*80)
    print("EVALUATING COMPLETIONS")
    print("="*80)
    print(f"Completions file: {completions_path}")

    # Load completions
    with open(completions_path, 'r') as f:
        completions_data = json.load(f)

    metadata = completions_data["metadata"]
    problems = completions_data["problems"]

    print(f"\nMetadata:")
    print(f"  Model: {metadata.get('model_id', 'unknown')}")
    print(f"  Model type: {metadata.get('model_type', 'unknown')}")
    print(f"  Adapter: {metadata.get('adapter_path', 'None')}")
    print(f"  Problems: {len(problems)}")
    print(f"  Samples per problem: {metadata.get('num_samples', 'unknown')}")
    print(f"  Temperature: {metadata.get('temperature', 'unknown')}")

    # Evaluate each completion
    eval_results = {
        "metadata": metadata,
        "problems": [],
        "summary": {}
    }

    total_samples = 0
    total_compiled = 0
    total_runtime_clean = 0
    problems_with_any_pass = 0

    print("\n" + "="*80)
    print("RUNNING EVALUATION")
    print("="*80)

    # Determine artifacts directory path
    if output_path is None:
        completions_filename = Path(completions_path).stem
        artifacts_base = Path(completions_path).parent / f"eval_{completions_filename}_artifacts"
    else:
        artifacts_base = Path(output_path).parent / f"{Path(output_path).stem}_artifacts"
    
    artifacts_base.mkdir(exist_ok=True)

    for problem_idx, problem in enumerate(problems):
        problem_name = problem["name"]
        
        # Handle two formats:
        # 1. Standard: problem["prompt"] + problem["completions"]
        # 2. Alternative: problem["samples"] where each sample has prompt + generated_code
        if "samples" in problem and len(problem["samples"]) > 0:
            # Format 2: Extract prompt from first sample (assuming all samples have same prompt)
            prompt = problem["samples"][0]["prompt"]
            tests = problem.get("tests", "")
            # Convert samples to completions format
            completions = [
                {
                    "sample_idx": sample.get("sample_idx", idx),
                    "generated_code": sample.get("generated_code", sample.get("full_code_for_compilation", ""))
                }
                for idx, sample in enumerate(problem["samples"])
            ]
        else:
            # Format 1: Standard format
            prompt = problem["prompt"]
            tests = problem.get("tests", "")
            completions = problem["completions"]

        print(f"\n[{problem_idx+1}/{len(problems)}] {problem_name}")
        print("-" * 80)

        problem_result = {
            "name": problem_name,
            "tests": tests,  # Preserve tests for potential re-evaluation
            "samples": [],
            "compile_rate": 0.0,
            "runtime_rate": 0.0,
            "passed_any": False
        }

        samples_compiled = 0
        samples_runtime_clean = 0

        for completion in completions:
            sample_idx = completion["sample_idx"]
            generated_code = completion["generated_code"]

            # Clean the generated code (remove main if present)
            cleaned_code = clean_generated_code(generated_code)

            # Sanitize tests string: remove leading closing brace ONLY if cleaned code already has it
            # Tests format is often "}\nint main() {..." but the } should come from generated code
            sanitized_tests = tests.strip()
            if sanitized_tests.startswith('}') and cleaned_code.rstrip().endswith('}'):
                # Remove the leading } and any whitespace/newline after it
                sanitized_tests = sanitized_tests[1:].lstrip()

            # Combine prompt + generated code + tests
            full_code = prompt + cleaned_code
            if sanitized_tests:
                full_code += "\n" + sanitized_tests

            # Sanitize problem name for filename (remove special chars)
            safe_problem_name = "".join(c if c.isalnum() or c in ('_', '-') else '_' for c in problem_name)
            src_filename = f"{safe_problem_name}_sample_{sample_idx}.cpp"
            src_path = str(artifacts_base / src_filename)

            # Compile and run (saves file as artifact)
            result = compile_and_run_code(full_code, src_path)

            total_samples += 1
            if result["compiled"]:
                samples_compiled += 1
                total_compiled += 1

            if result["runtime_clean"]:
                samples_runtime_clean += 1
                total_runtime_clean += 1

            # Store result
            sample_result = {
                "sample_idx": sample_idx,
                "prompt": prompt,
                "generated_code": generated_code,
                "full_code_for_compilation": full_code,
                "src_file": result["src_file"],
                "compiled": result["compiled"],
                "runtime_clean": result["runtime_clean"],
            }

            if "exe_file" in result:
                sample_result["exe_file"] = result["exe_file"]
            if "compile_error" in result:
                sample_result["compile_error"] = result["compile_error"]
            if "runtime_error" in result:
                sample_result["runtime_error"] = result["runtime_error"]

            problem_result["samples"].append(sample_result)

            # Progress indicator
            status = "??" if result["runtime_clean"] else ("??" if result["compiled"] else "??")
            print(f"  Sample {sample_idx+1}/{len(completions)}: {status}", end="\r")

        print()  # New line after progress

        # Problem-level metrics
        problem_result["compile_rate"] = samples_compiled / len(completions)
        problem_result["runtime_rate"] = samples_runtime_clean / len(completions)
        problem_result["passed_any"] = samples_runtime_clean > 0

        if problem_result["passed_any"]:
            problems_with_any_pass += 1

        print(f"  Compile: {samples_compiled}/{len(completions)} ({problem_result['compile_rate']:.1%})")
        print(f"  Runtime Clean: {samples_runtime_clean}/{len(completions)} ({problem_result['runtime_rate']:.1%})")

        eval_results["problems"].append(problem_result)

    # Calculate summary metrics
    num_samples_per_problem = metadata.get("num_samples", 10)

    # Calculate pass@k for different k values
    pass_at_k_metrics = {}
    for k in [1, 5, 10]:
        if k <= num_samples_per_problem:
            # Average pass@k across all problems
            pass_at_k_values = []
            for problem_result in eval_results["problems"]:
                n = len(problem_result["samples"])
                c = sum(1 for s in problem_result["samples"] if s["runtime_clean"])
                pass_at_k_values.append(calculate_pass_at_k(n, c, k))

            pass_at_k_metrics[f"pass@{k}"] = sum(pass_at_k_values) / len(pass_at_k_values) if pass_at_k_values else 0

    eval_results["summary"] = {
        "total_samples": total_samples,
        "compile_success_rate": total_compiled / total_samples if total_samples > 0 else 0,
        "runtime_success_rate": total_runtime_clean / total_samples if total_samples > 0 else 0,
        "problems_with_any_pass": problems_with_any_pass,
        "problems_with_any_pass_rate": problems_with_any_pass / len(problems) if len(problems) > 0 else 0,
        **pass_at_k_metrics
    }

    # Print summary
    print("\n" + "="*80)
    print("EVALUATION SUMMARY")
    print("="*80)
    print(f"Total samples: {total_samples}")
    print(f"Compilation success rate: {eval_results['summary']['compile_success_rate']:.2%}")
    print(f"Runtime success rate (ASan/UBSan clean): {eval_results['summary']['runtime_success_rate']:.2%}")
    for k, v in pass_at_k_metrics.items():
        print(f"{k}: {v:.2%}")
    print(f"Problems with ?1 passing sample: {problems_with_any_pass}/{len(problems)} ({eval_results['summary']['problems_with_any_pass_rate']:.2%})")
    print("="*80)

    # Save results
    if output_path is None:
        # Auto-generate output path
        completions_filename = Path(completions_path).stem
        output_path = str(Path(completions_path).parent / f"eval_{completions_filename}.json")

    with open(output_path, "w") as f:
        json.dump(eval_results, f, indent=2)

    print(f"\n? Results saved to: {output_path}")
    print(f"? Code artifacts saved to: {artifacts_base}")

    return eval_results


def main():
    """
    Main CLI for evaluation.

    Usage:
        python training/eval_completions.py <completions.json> [output.json]

    Examples:
        # Evaluate completions (auto-generates output filename)
        python training/eval_completions.py ./completions_base.json

        # Evaluate with custom output path
        python training/eval_completions.py ./completions_base.json ./eval_results.json
    """
    if len(sys.argv) < 2:
        print("Usage: python training/eval_completions.py <completions.json> [output.json]")
        sys.exit(1)

    completions_path = sys.argv[1]
    output_path = sys.argv[2] if len(sys.argv) > 2 else None

    if not os.path.exists(completions_path):
        print(f"Error: Completions file not found: {completions_path}")
        sys.exit(1)

    evaluate_completions(completions_path, output_path)


if __name__ == "__main__":
    main()
